# chapter01

## 1.3 Slicing Operation

If you want to slice it, best to use `x.slice(dim,start,end,step)`.

If you want to index it, best to use `x.index()`.

> I say, but don't have to say.:broken_heart::broken_heart::broken_heart:

### 1.3.1 Basic

Example:

```cpp
void SlicingOperation_Basic()
{
	auto x = torch::rand({ 3, 3, 5 });
	cout << x << endl;
	
	// Index data like std::vector use operator [].
	cout << "channel 0:x[0]\n" << x[0] << endl;
	cout << "channel 1:x[0][0]\n" << x[0][0] << endl;
	cout << "channel 2:x[0][0][0]\n" << x[0][0][0] << endl;
}
```

Output:

```cpp
(1,.,.) =
  0.8927  0.4161  0.6482  0.2300  0.2978
  0.9934  0.6573  0.2763  0.4752  0.1418
  0.1038  0.9200  0.6195  0.7264  0.1310

(2,.,.) =
  0.1220  0.7370  0.0485  0.8778  0.1804
  0.1810  0.3222  0.0747  0.4049  0.2009
  0.7939  0.2031  0.6824  0.8598  0.6659

(3,.,.) =
  0.1224  0.7043  0.3316  0.9640  0.6293
  0.5329  0.1876  0.2530  0.5470  0.2826
  0.7711  0.6882  0.1407  0.9074  0.9864
[ CPUFloatType{3,3,5} ]
channel 0:x[0]
 0.8927  0.4161  0.6482  0.2300  0.2978
 0.9934  0.6573  0.2763  0.4752  0.1418
 0.1038  0.9200  0.6195  0.7264  0.1310
[ CPUFloatType{3,5} ]
channel 1:x[0][0]
 0.8927
 0.4161
 0.6482
 0.2300
 0.2978
[ CPUFloatType{5} ]
channel 2:x[0][0][0]
0.892736
[ CPUFloatType{} ]
```

### 1.3.2 slice()

- Function Document:

  - [at::Tensor slice(int64_t dim = 0, c10::optional<int64_t> start = c10::nullopt, c10::optional<int64_t> end = c10::nullopt, int64_t step = 1) const](https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor5sliceE7int64_tN3c108optionalI7int64_tEEN3c108optionalI7int64_tEE7int64_t)

- Example:

  ```cpp
  // slice(dim,start,end,step);shallow copy.
  auto x = torch::rand({ 3, 3, 5 });
  cout << x << endl;
  cout << x.slice(1, 1) << endl;
  ```

- Output:

  ```cpp
  (1,.,.) =
    0.7905  0.1390  0.6868  0.0434  0.9115
    0.7385  0.5464  0.1769  0.8056  0.6902
    0.1892  0.5764  0.7432  0.4332  0.4660
  
  (2,.,.) =
    0.8078  0.4808  0.7757  0.6373  0.7056
    0.6093  0.2788  0.4533  0.9486  0.0435
    0.3261  0.1759  0.0233  0.8778  0.6732
  
  (3,.,.) =
    0.6369  0.1941  0.4619  0.5826  0.0621
    0.8111  0.0754  0.6370  0.7951  0.0393
    0.3818  0.2151  0.3777  0.3870  0.9996
  [ CPUFloatType{3,3,5} ]
  (1,.,.) =
    0.7385  0.5464  0.1769  0.8056  0.6902
    0.1892  0.5764  0.7432  0.4332  0.4660
  
  (2,.,.) =
    0.6093  0.2788  0.4533  0.9486  0.0435
    0.3261  0.1759  0.0233  0.8778  0.6732
  
  (3,.,.) =
    0.8111  0.0754  0.6370  0.7951  0.0393
    0.3818  0.2151  0.3777  0.3870  0.9996
  [ CPUFloatType{3,2,5} ]
  ```

### 1.3.3 index()

- Function Document:

  - [at::Tensor index(const c10::List<c10::optional<at::Tensor>> &indices) const](https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor5indexERKN3c104ListIN3c108optionalIN2at6TensorEEEEE)

  Create a sample variable

  ```cpp
  auto x = torch::rand({ 3, 3, 5 });
  cout << x << endl
  ```

  Output:

  ```cpp
  (1,.,.) =
    0.3027  0.6006  0.3196  0.4545  0.9706
    0.7210  0.3467  0.0611  0.2126  0.4365
    0.2238  0.1902  0.3374  0.2795  0.1802
  
  (2,.,.) =
    0.3128  0.8110  0.1262  0.3599  0.5501
    0.4152  0.0703  0.3672  0.8825  0.0227
    0.2355  0.0051  0.5505  0.7298  0.9806
  
  (3,.,.) =
    0.2881  0.9329  0.3070  0.0163  0.5192
    0.2852  0.7986  0.3838  0.7721  0.8707
    0.6611  0.6106  0.9556  0.8749  0.4332
  [ CPUFloatType{3,3,5} ]
  ```

- **x.index({int, int, ...})**

  Example:

  ```cpp
  // Method 1: x.index({int, int, ...})
  //	This way is suitable to fetch only one dimension of data.
  cout << x.index({ 0, 1, 1 }) << endl;
  vector<int> v{ 1,2 };
  //		But x.index(vector) or x.index({vector}) both are error.
  ```

  Output:

  ```cpp
  0.34673
  [ CPUFloatType{} ]
  ```

- **x.index({tensor})**

  Example:

  ```cpp
  //	Way 1: x.index({tensor})
  //		This way returns a sequence in dim 0.
  auto idx1 = torch::tensor({ 0,1 });
  cout << "x.index({tensor}):\n" << x.index({idx1}) << endl;
  //		But x.index(tensor) is error.
  ```

  Output:

  ```cpp
  x.index({tensor}):
  (1,.,.) =
    0.3027  0.6006  0.3196  0.4545  0.9706
    0.7210  0.3467  0.0611  0.2126  0.4365
    0.2238  0.1902  0.3374  0.2795  0.1802
  
  (2,.,.) =
    0.3128  0.8110  0.1262  0.3599  0.5501
    0.4152  0.0703  0.3672  0.8825  0.0227
    0.2355  0.0051  0.5505  0.7298  0.9806
  [ CPUFloatType{2,3,5} ]
  ```

- **x.index({tensor, int})**

  Example:

  ```cpp
  //	Way 2: x.index({tensor, int})
  //		This way returns a sequence tensor in dim 0, and int in dim 1.
  cout << "x.index({tensor, int}):\n" << x.index({idx1, 1}) << endl;
  ```

  Output:

  ```cpp
  x.index({tensor, int}):
   0.7210  0.3467  0.0611  0.2126  0.4365
   0.4152  0.0703  0.3672  0.8825  0.0227
  [ CPUFloatType{2,5} ]
  ```

- **x.index({tensor1,tensor2})**

  Example:

  ```cpp
  //	Way 3: x.index({tensor1,tensor2})
  //		This way returns a sequence [tensor1[0] in dim 0, tensor2[0] in dim 1],
  //							a sequence [tensor1[1] in dim 0, tensor2[1] in dim 1]...
  cout << "x.index({tensor1,tensor2}):\n" << x.index({idx1, idx1}) << endl;
  auto idx2 = torch::tensor({ 0,1,2 });
  //		So tensor1.sizes() must same as tensor2.sizes(), otherwise error.
  //cout << x.index({ idx1, idx2 }) << endl;		//error
  ```

  Output:

  ```cpp
  x.index({tensor1,tensor2}):
   0.3027  0.6006  0.3196  0.4545  0.9706
   0.4152  0.0703  0.3672  0.8825  0.0227
  [ CPUFloatType{2,5} ]
  ```

- **x.index({"...", int})**

  Example:

  ```cpp
  // Way 4: x.index({"...", int})
  //		This way returns a sequence all but int in dim last.Just last.
  //		And you will find that cols -> rows.
  cout << "x.index({\"...\", int}):\n" << x.index({ "...", 1 }) << endl;
  ```

  Output:

  ```cpp
   0.6006  0.3467  0.1902
   0.8110  0.0703  0.0051
   0.9329  0.7986  0.6106
  [ CPUFloatType{3,3} ]
  ```

- **All Code**

  ```cpp
  void SlicingOperation_Index()
  {
  	// There are many methods to call.
  	auto x = torch::rand({ 3, 3, 5 });
  	cout << x << endl;
  
  	// Method 1: x.index({int, int, ...})
  	//	This way is suitable to fetch only one dimension of data.
  	cout << x.index({ 0, 1, 1 }) << endl;
  	vector<int> v{ 1,2 };
  	//		But x.index(vector) or x.index({vector}) both are error.
  
  	cout << endl << "*************************" << endl;
  
  	// Method 2: x.index({tensor}), but has many ways to call.
  	//	Way 1: x.index({tensor})
  	//		This way returns a sequence in dim 0.
  	auto idx1 = torch::tensor({ 0,1 });
  	cout << "x.index({tensor}):\n" << x.index({idx1}) << endl;
  	//		But x.index(tensor) is error.
  
  	//	Way 2: x.index({tensor, int})
  	//		This way returns a sequence tensor in dim 0, and int in dim 1.
  	cout << "x.index({tensor, int}):\n" << x.index({idx1, 1}) << endl;
  
  	//	Way 3: x.index({tensor1,tensor2})
  	//		This way returns a sequence [tensor1[0] in dim 0, tensor2[0] in dim 1],
  	//							a sequence [tensor1[1] in dim 0, tensor2[1] in dim 1]...
  	cout << "x.index({tensor1,tensor2}):\n" << x.index({idx1, idx1}) << endl;
  	auto idx2 = torch::tensor({ 0,1,2 });
  	//		So tensor1.sizes() must same as tensor2.sizes(), otherwise error.
  	//cout << x.index({ idx1, idx2 }) << endl;		//error
  
  	// Way 4: x.index({"...", int})
  	//		This way returns a sequence all but int in dim last.Just last.
  	//		And you will find that cols -> rows.
  	cout << "x.index({\"...\", int}):\n" << x.index({ "...", 1 }) << endl;
  }
  ```

  

### All Code

```cpp
void SlicingOperation_Basic()
{
	auto x = torch::rand({ 3, 3, 5 });
	cout << x << endl;
	
	// Index data like std::vector use operator [].
	cout << "channel 0:x[0]\n" << x[0] << endl;
	cout << "channel 1:x[0][0]\n" << x[0][0] << endl;
	cout << "channel 2:x[0][0][0]\n" << x[0][0][0] << endl;
}


void SlicingOperation_Slice()
{
	// slice(dim,start,end,step);shallow copy.
	auto x = torch::rand({ 3, 3, 5 });
	cout << x << endl;
	cout << x.slice(1, 1) << endl;
	
	//auto y = x.slice(1, 1);
	//y[0] = 3;
	//cout << x << endl;
}

void SlicingOperation_Index()
{
	// There are many methods to call.
	auto x = torch::rand({ 3, 3, 5 });
	cout << x << endl;

	// Method 1: x.index({int, int, ...})
	//	This way is suitable to fetch only one dimension of data.
	cout << x.index({ 0, 1, 1 }) << endl;
	vector<int> v{ 1,2 };
	//		But x.index(vector) or x.index({vector}) both are error.

	cout << endl << "*************************" << endl;

	// Method 2: x.index({tensor}), but has many ways to call.
	//	Way 1: x.index({tensor})
	//		This way returns a sequence in dim 0.
	auto idx1 = torch::tensor({ 0,1 });
	cout << "x.index({tensor}):\n" << x.index({idx1}) << endl;
	//		But x.index(tensor) is error.

	//	Way 2: x.index({tensor, int})
	//		This way returns a sequence tensor in dim 0, and int in dim 1.
	cout << "x.index({tensor, int}):\n" << x.index({idx1, 1}) << endl;

	//	Way 3: x.index({tensor1,tensor2})
	//		This way returns a sequence [tensor1[0] in dim 0, tensor2[0] in dim 1],
	//							a sequence [tensor1[1] in dim 0, tensor2[1] in dim 1]...
	cout << "x.index({tensor1,tensor2}):\n" << x.index({idx1, idx1}) << endl;
	auto idx2 = torch::tensor({ 0,1,2 });
	//		So tensor1.sizes() must same as tensor2.sizes(), otherwise error.
	//cout << x.index({ idx1, idx2 }) << endl;		//error

	// Way 4: x.index({"...", int})
	//		This way returns a sequence all but int in dim last.Just last.
	//		And you will find that cols -> rows.
	cout << "x.index({\"...\", int}):\n" << x.index({ "...", 1 }) << endl;
}

void SlicingOperation_Select()
{
	// select(dim,index)
	// Select is very easy, not suitable for dim >= 2;
	//	If dim >=2 , returns every index in dim.
	//	If you want to copy, add '.clone()'.
	auto x = torch::rand({ 3, 3, 5 });
	cout << x << endl;
	cout << x.select(1, 1) << endl;
	auto x1 = torch::select(x, 1, 2);
	cout << x1 << endl;
}

void SlicingOperation_IndexSelect()
{
	auto x = torch::rand({ 3, 3, 5 });
	cout << x << endl;
	auto idx = torch::tensor({ 0,1 });
	cout << x.index_select(1, idx) << endl;
}
```

