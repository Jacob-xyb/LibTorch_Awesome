# chapter01

## 1.3 Slicing Operation

### 1.3.1 Basic

Example:

```cpp
void SlicingOperation_Basic()
{
	auto x = torch::rand({ 3, 3, 5 });
	cout << x << endl;
	
	// Index data like std::vector use operator [].
	cout << "channel 0:x[0]\n" << x[0] << endl;
	cout << "channel 1:x[0][0]\n" << x[0][0] << endl;
	cout << "channel 2:x[0][0][0]\n" << x[0][0][0] << endl;
}
```

Output:

```cpp
(1,.,.) =
  0.8927  0.4161  0.6482  0.2300  0.2978
  0.9934  0.6573  0.2763  0.4752  0.1418
  0.1038  0.9200  0.6195  0.7264  0.1310

(2,.,.) =
  0.1220  0.7370  0.0485  0.8778  0.1804
  0.1810  0.3222  0.0747  0.4049  0.2009
  0.7939  0.2031  0.6824  0.8598  0.6659

(3,.,.) =
  0.1224  0.7043  0.3316  0.9640  0.6293
  0.5329  0.1876  0.2530  0.5470  0.2826
  0.7711  0.6882  0.1407  0.9074  0.9864
[ CPUFloatType{3,3,5} ]
channel 0:x[0]
 0.8927  0.4161  0.6482  0.2300  0.2978
 0.9934  0.6573  0.2763  0.4752  0.1418
 0.1038  0.9200  0.6195  0.7264  0.1310
[ CPUFloatType{3,5} ]
channel 1:x[0][0]
 0.8927
 0.4161
 0.6482
 0.2300
 0.2978
[ CPUFloatType{5} ]
channel 2:x[0][0][0]
0.892736
[ CPUFloatType{} ]
```

### 1.3.2 Index

- Function Document:

  - [at::Tensor index(const c10::List<c10::optional<at::Tensor>> &indices) const](https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor5indexERKN3c104ListIN3c108optionalIN2at6TensorEEEEE)

  Create a sample variable

  ```cpp
  auto x = torch::rand({ 3, 3, 5 });
  cout << x << endl
  ```

  Output:

  ```cpp
  (1,.,.) =
    0.3027  0.6006  0.3196  0.4545  0.9706
    0.7210  0.3467  0.0611  0.2126  0.4365
    0.2238  0.1902  0.3374  0.2795  0.1802
  
  (2,.,.) =
    0.3128  0.8110  0.1262  0.3599  0.5501
    0.4152  0.0703  0.3672  0.8825  0.0227
    0.2355  0.0051  0.5505  0.7298  0.9806
  
  (3,.,.) =
    0.2881  0.9329  0.3070  0.0163  0.5192
    0.2852  0.7986  0.3838  0.7721  0.8707
    0.6611  0.6106  0.9556  0.8749  0.4332
  [ CPUFloatType{3,3,5} ]
  ```

- **x.index({int, int, ...})**

  Example:

  ```cpp
  // Method 1: x.index({int, int, ...})
  //	This way is suitable to fetch only one dimension of data.
  cout << x.index({ 0, 1, 1 }) << endl;
  vector<int> v{ 1,2 };
  //		But x.index(vector) or x.index({vector}) both are error.
  ```

  Output:

  ```cpp
  0.34673
  [ CPUFloatType{} ]
  ```

- **x.index({tensor})**

  Example:

  ```cpp
  //	Way 1: x.index({tensor})
  //		This way returns a sequence in dim 0.
  auto idx1 = torch::tensor({ 0,1 });
  cout << "x.index({tensor}):\n" << x.index({idx1}) << endl;
  //		But x.index(tensor) is error.
  ```

  Output:

  ```cpp
  x.index({tensor}):
  (1,.,.) =
    0.3027  0.6006  0.3196  0.4545  0.9706
    0.7210  0.3467  0.0611  0.2126  0.4365
    0.2238  0.1902  0.3374  0.2795  0.1802
  
  (2,.,.) =
    0.3128  0.8110  0.1262  0.3599  0.5501
    0.4152  0.0703  0.3672  0.8825  0.0227
    0.2355  0.0051  0.5505  0.7298  0.9806
  [ CPUFloatType{2,3,5} ]
  ```

- **x.index({tensor, int})**

  Example:

  ```cpp
  //	Way 2: x.index({tensor, int})
  //		This way returns a sequence tensor in dim 0, and int in dim 1.
  cout << "x.index({tensor, int}):\n" << x.index({idx1, 1}) << endl;
  ```

  Output:

  ```cpp
  x.index({tensor, int}):
   0.7210  0.3467  0.0611  0.2126  0.4365
   0.4152  0.0703  0.3672  0.8825  0.0227
  [ CPUFloatType{2,5} ]
  ```

- **x.index({tensor1,tensor2})**

  Example:

  ```cpp
  //	Way 3: x.index({tensor1,tensor2})
  //		This way returns a sequence [tensor1[0] in dim 0, tensor2[0] in dim 1],
  //							a sequence [tensor1[1] in dim 0, tensor2[1] in dim 1]...
  cout << "x.index({tensor1,tensor2}):\n" << x.index({idx1, idx1}) << endl;
  auto idx2 = torch::tensor({ 0,1,2 });
  //		So tensor1.sizes() must same as tensor2.sizes(), otherwise error.
  //cout << x.index({ idx1, idx2 }) << endl;		//error
  ```

  Output:

  ```cpp
  x.index({tensor1,tensor2}):
   0.3027  0.6006  0.3196  0.4545  0.9706
   0.4152  0.0703  0.3672  0.8825  0.0227
  [ CPUFloatType{2,5} ]
  ```

- **x.index({"...", int})**

  Example:

  ```cpp
  // Way 4: x.index({"...", int})
  //		This way returns a sequence all but int in dim last.Just last.
  //		And you will find that cols -> rows.
  cout << "x.index({\"...\", int}):\n" << x.index({ "...", 1 }) << endl;
  ```

  Output:

  ```cpp
   0.6006  0.3467  0.1902
   0.8110  0.0703  0.0051
   0.9329  0.7986  0.6106
  [ CPUFloatType{3,3} ]
  ```

- **All Code**

  ```cpp
  void SlicingOperation_Index()
  {
  	// There are many methods to call.
  	auto x = torch::rand({ 3, 3, 5 });
  	cout << x << endl;
  
  	// Method 1: x.index({int, int, ...})
  	//	This way is suitable to fetch only one dimension of data.
  	cout << x.index({ 0, 1, 1 }) << endl;
  	vector<int> v{ 1,2 };
  	//		But x.index(vector) or x.index({vector}) both are error.
  
  	cout << endl << "*************************" << endl;
  
  	// Method 2: x.index({tensor}), but has many ways to call.
  	//	Way 1: x.index({tensor})
  	//		This way returns a sequence in dim 0.
  	auto idx1 = torch::tensor({ 0,1 });
  	cout << "x.index({tensor}):\n" << x.index({idx1}) << endl;
  	//		But x.index(tensor) is error.
  
  	//	Way 2: x.index({tensor, int})
  	//		This way returns a sequence tensor in dim 0, and int in dim 1.
  	cout << "x.index({tensor, int}):\n" << x.index({idx1, 1}) << endl;
  
  	//	Way 3: x.index({tensor1,tensor2})
  	//		This way returns a sequence [tensor1[0] in dim 0, tensor2[0] in dim 1],
  	//							a sequence [tensor1[1] in dim 0, tensor2[1] in dim 1]...
  	cout << "x.index({tensor1,tensor2}):\n" << x.index({idx1, idx1}) << endl;
  	auto idx2 = torch::tensor({ 0,1,2 });
  	//		So tensor1.sizes() must same as tensor2.sizes(), otherwise error.
  	//cout << x.index({ idx1, idx2 }) << endl;		//error
  
  	// Way 4: x.index({"...", int})
  	//		This way returns a sequence all but int in dim last.Just last.
  	//		And you will find that cols -> rows.
  	cout << "x.index({\"...\", int}):\n" << x.index({ "...", 1 }) << endl;
  }
  ```

  

