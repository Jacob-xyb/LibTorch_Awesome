# chapter01

## 1.2 Tensor Reshape

`LibTorch` reshapes in the same way as `PyTorch`, like `view`，`transpose`，`reshape`，`permute` and so on.

Update the most commonly used, the rest follows.

### 1.2.1 View / Reshape

`View` is exactly the same as `Reshape`.

Because PyTorch use view first, but Numpy use reshape. So to be consistent, LibTorch implement the reshape api same as view.

---

Function Document:

- [at::Tensor view(at::IntArrayRef size) const](https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor4viewEN2at11IntArrayRefE)

- [at::Tensor view(at::ScalarType dtype) const](https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor4viewEN2at10ScalarTypeE)

- [at::Tensor view_as(const at::Tensor &other) const](https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor7view_asERKN2at6TensorE)

- [at::Tensor reshape(at::IntArrayRef shape) const](https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor7reshapeEN2at11IntArrayRefE)

- [at::Tensor reshape_as(const at::Tensor &other) const](https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor10reshape_asERKN2at6TensorE)

Example:

```cpp
// View and reshape neither is in-place, please use 'data = data.view()/data.reshape()'.
auto x = torch::arange(9);
cout << "x.view({3,3}):\n" << x.view({ 3,3 }) << endl;
cout << "x.reshape({3,3}):\n" << x.reshape({3,3}) << endl;

auto y = x.view({ 3,3 });
cout << "view_as():\n" << x.view_as(y) << endl;
```

Output:

```cpp
x.view({3,3}):
 0  1  2
 3  4  5
 6  7  8
[ CPULongType{3,3} ]
x.reshape({3,3}):
 0  1  2
 3  4  5
 6  7  8
[ CPULongType{3,3} ]
view_as():
 0  1  2
 3  4  5
 6  7  8
[ CPULongType{3,3} ]
```

- **Tips:**

View and reshape both are shallow copy.

> I can't find the data's memory address of Tensor, only display by this way.

```cpp
// View and reshape both are shallow copy.
y[0] = torch::tensor({ 8, 8, 8 });
cout << x.view({ 3,3 }) << endl;

auto z = x.reshape({ 3,3 });
z[1] = torch::tensor({ 6, 6, 6 });
cout << z.view({ 3,3 }) << endl;
```

Output:

```cpp
 8  8  8
 3  4  5
 6  7  8
[ CPULongType{3,3} ]
 8  8  8
 6  6  6
 6  7  8
[ CPULongType{3,3} ]
```

### 1.2.2 .transpose()

- **.t() \ .transpose()**

  `.t()` and `.transpose()` both are shallow copy.

  Function Document:

  - [at::Tensor transpose(int64_t dim0, int64_t dim1) const](https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor9transposeE7int64_t7int64_t)
  - [at::Tensor transpose(at::Dimname dim0, at::Dimname dim1) const](https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor9transposeE7int64_t7int64_t)

  Example:

  ```cpp
  // `.t()` and `.transpose()` both are shallow copy.
  auto x1 = torch::rand({ 2,3 });
  cout << x1 << endl;
  auto x2 = x1.t();
  cout << x2 << endl;
  auto x3 = x1.transpose(0,1);
  cout << x3 << endl;
  
  TORCH_CHECK(x1.data_ptr() == x2.data_ptr());
  TORCH_CHECK(x1.data_ptr() == x3.data_ptr());
  ```

  Output:

  ```cpp
   0.0357  0.4230  0.0833
   0.5323  0.0642  0.2425
  [ CPUFloatType{2,3} ]
   0.0357  0.5323
   0.4230  0.0642
   0.0833  0.2425
  [ CPUFloatType{3,2} ]
   0.0357  0.5323
   0.4230  0.0642
   0.0833  0.2425
  [ CPUFloatType{3,2} ]
  ```

- **Tips:**

  But you need to know, after `.t()` and `.transpose()`, data is not contiguous in storage.

  You can't use `view()`.

  ```cpp
  // error
  cout << x2.view({ 1,6 }) << endl;
  cout << x3.view({ 1,6 }) << endl;
  ```

- **.is_contiguous() \ .contiguous()**

  ```cpp
  // First determine whether it is continuous. `.is_contiguous()`.
  cout << "after .t(), x2.is_contiguous(): " << x2.is_contiguous() << endl;
  x2 = x2.contiguous();
  //	Notice now that the memory space has changed.
  // 	   Means that `.contiguous()` is deep copy.
  //TORCH_CHECK(x1.data_ptr() == x2.data_ptr());
  cout << "after .contiguous(), x2.is_contiguous(): " << x2.is_contiguous() << endl;
  cout << x2.view({ 1,6 }) << endl;
  ```

  Output:

  ```cpp
  after .t(), x2.is_contiguous(): 0
  after .contiguous(), x2.is_contiguous(): 1
   0.0357  0.5323  0.4230  0.0642  0.0833  0.2425
  [ CPUFloatType{1,6} ]
  ```

### 1.2.3 .permute()

> swaps all axis at once.

`permute()` is similar to `transpose()`.

transpose() has 2 ways to call: `torch::transpose(Tensor, a,b)` 、`Tensor.transpose(a,b)` .

but `permute()` has only 1 way to call: `Tensor.permute(dimArray)` .

__And transpose() only change 2 dims.__

- Function Document:

  [at::Tensor permute(at::IntArrayRef dims) const](https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#_CPPv4NK2at6Tensor7permuteEN2at11IntArrayRefE)

- Example:

  ```cpp
  void TensorReshape_permute()
  {
  	auto x = torch::ones({ 1,2,3,4 });
  	cout << x.sizes() << endl;
  	auto y = x.permute({ 2,3,0,1 });
  	cout << y.sizes() << endl;
  	TORCH_CHECK(x.data_ptr() == y.data_ptr());
  }
  ```

- Output:

  ```cpp
  [1, 2, 3, 4]
  [3, 4, 1, 2]
  ```

  